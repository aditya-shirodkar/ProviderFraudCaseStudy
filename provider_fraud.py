# -*- coding: utf-8 -*-
"""Provider Fraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ir5pQYHFf9A32Vj9e7HOOeEWS46Axjf8

###Libraries
"""

# !pip3 install catboost

import collections
import glob

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# from catboost import CatBoostClassifier
# from imblearn.over_sampling import RandomOverSampler
# from lightgbm import LGBMClassifier
from sklearn.decomposition import PCA
# from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
# from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
# from sklearn.svm import SVC
from xgboost import XGBClassifier

"""### Loading and Transforming Data"""

files = glob.glob("*.csv")  # please put all files, Train and Unseen, in the root folder
files.sort()
dfs = []
for file in files:
    dfs.append(pd.read_csv(file))

len(dfs)

files

"""Let"s not make too many intermediate DataFrames in the interest of time. Let"s make training and testing datasets before cleaning."""

dfs[0].head()

dfs[0]["PotentialFraud"].value_counts()

dfs[1].head()

dfs[2].head()

dfs[3].head()

dfs[2].shape

dfs[3].shape

unique_cols_inpatient = set(dfs[2].columns) - set(dfs[3].columns)
unique_cols_inpatient

unique_cols_outpatient = set(dfs[3].columns) - set(dfs[2].columns)
unique_cols_outpatient

percent_inpatients = len(dfs[2])*100 / (len(dfs[2]) + len(dfs[3]))
print(f"Percentage of inpatients: {percent_inpatients:.2f}%")

"""Given that the vast majority of patients are outpatients, we are going to ignore the columns of the inpatients for the overall training, although admission length might be a valuable feature. A single flag calling a patient "Inpatient" and "Outpatient" should hopefully account for the data in these columns."""

dfs[2]["BeneType"] = "Inpatient"
dfs[3]["BeneType"] = "Outpatient"

df_patients = pd.concat([dfs[2].drop(columns=unique_cols_inpatient), dfs[3]], axis=0)
df_patients.head()

df_patients["Provider"].nunique()

len(dfs[0])

dfs[0]["Provider"].nunique()

df_patients["BeneID"].nunique()

len(dfs[1])

"""Let"s construct the training dataset by merging benefactor and provider information, given that they appear to be keys"""

df_train = df_patients.merge(dfs[1], how="left", on="BeneID").merge(dfs[0], how="left", on="Provider").reset_index(drop=True)
df_train.head()

df_train.info()

"""Checking to see if 'Provider' is a unique key"""

df_train['Provider'].nunique()

len(dfs[0])

"""The potential fraud is provided on a provider level, not a claim level. We need to perform our predictions on a claim level.

### Exploratory Data Analysis
"""

df_plot = df_train.copy()
df_plot.loc[df_plot["PotentialFraud"] == "No", "PotentialFraud"] = 0
df_plot.loc[df_plot["PotentialFraud"] == "Yes", "PotentialFraud"] = 1
df_plot["PotentialFraud"] = df_plot["PotentialFraud"].astype("int64")

# transforming some columns to categorical
cols_code = [col for col in df_plot.columns if "Code" in col]
df_plot[cols_code + ["State", "County"]] = df_plot[cols_code + ["State", "County"]].astype(str)

correlation_matrix = df_plot.select_dtypes("number").corr()

plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, cmap="coolwarm")
plt.show()

"""No real significant correlations other than natural ones like reimbursement/deductible amounts. No strong correlation with PotentialFraud."""

df_plot["PotentialFraud"].value_counts()

plt.figure(figsize=(6, 4))
plt.pie(df_plot["PotentialFraud"].value_counts(), labels=["No", "Yes"], autopct='%1.0f%%')
plt.title("Proportion of claims marked as fraud")

plt.show()
print()
plt.figure(figsize=(6, 4))
plt.pie(dfs[0]["PotentialFraud"].value_counts(), labels=["No", "Yes"])
plt.title("Proportion of providers marked as fraud")
plt.show()

"""In the dataset, the proportion of claims marked as fraud are around 38%, while the number of providers marked as fraud are around 9%. This tells us that the dataset is biased to feature more claims by likely fraudster providers. The goal must then be to try to extract other information such as benefactor, physician, and code information from these fraud claims."""

df_plot.loc[df_plot["PotentialFraud"]==1, :]

plt.figure(figsize=(6, 4))
sns.boxplot(data=df_plot, y="InscClaimAmtReimbursed", x="PotentialFraud")
plt.title("Boxplot of insurance claim reimbursement amount per fraud category")
plt.show()

"""Most disbursed amounts are tiny, and there is a large outlier tail (outliers classified by IQR method)."""

plt.figure(figsize=(12, 8))
sns.barplot(data=df_plot, x="State", y="PotentialFraud", order=df_plot["State"].astype(int).sort_values().astype(str).unique())
plt.title("Potential fraud by state")
plt.show()

"""Likelihood of fraud by state reveals that the state numbered 46 has the highest likelihood of fraud and/or that this state is highly represented in the database."""

plt.figure(figsize=(6, 4))
sns.barplot(data=df_plot, x="Gender", y="PotentialFraud")
plt.title("Potential fraud by gender")
plt.show()

"""### Data Cleaning and Feature Engineering"""

df_train.shape

len(df_train["ClaimID"].unique())

"""Therefore, we are considering the claims to be the index for our table."""

df_train.set_index("ClaimID", inplace=True)

df_train.info()

"""Let"s find the missing percentage of each column."""

df_missing_percent = pd.DataFrame({"Missing %": df_train.isna().sum()*100/len(df_train)}, index=df_train.columns)
df_missing_percent

"""Let"s deal with the columns step-by-step:

#### Code and Physician Columns

Admit diagnosis, diagnosis, and procedure codes seem like categoricals. Furthermore, for the latter two, there is a sequential falloff in null values. The patients may have multiple different diagnoses. Let"s target this first as it"s the hardest bunch of columns to deal with. Features we can engineer out of these:

*   Count of number of diagnoses per claim
*   Count of number of procedures per claim
*   Check to see if any admit diagnoses and diagnoses match
*   One-hot-encoding of most common admit diagnoses; most common admit diagnoses where fraud was detected
*   One-hot-encoding of most common diagnoses; most common diagnoses where fraud was detected
*   One-hot-encoding of most common procedures; most common procedures where fraud was detected

Similarly for physicians:

*   Count of number of physicians per claim
*   Check to see if any of the physicians (attending, operating, other) match
*   One-hot-encoding of most common physicians
*   One-hot-encoding of most common physicians where fraud was detected, most common physicians where fraud was detected

Then we can drop the columns
"""

cols_code = [col for col in df_train.columns if "Code" in col]
cols_code

cols_physician = [col for col in df_train.columns if "Physician" in col]
cols_physician

col_admit_diagnosis = "ClmAdmitDiagnosisCode"
cols_diagnosis = [col for col in cols_code if "ClmDiagnosisCode_" in col]
cols_procedure = [col for col in cols_code if "ClmProcedureCode_" in col]

"""Let"s count the number of diagnoses and procedures, and if the admit diagnosis and any diagnosis matches."""

df_train["NumDiagnosis"] = df_train[cols_diagnosis].count(axis=1)
df_train["NumProcedures"] = df_train[cols_procedure].count(axis=1)
df_train["NumPhysicians"] = df_train[cols_physician].count(axis=1)
df_train["DiagnosisMatch"] = df_train[cols_diagnosis].isin(df_train[col_admit_diagnosis]).any(axis=1)
df_train["PhysicianMatch"] = df_train[cols_physician].nunique(axis=1) < 3

"""Let"s verify these columns with a sample"""

df_train[cols_code + ["NumDiagnosis", "NumProcedures", "DiagnosisMatch"]].head()

"""Let"s make one-hot-encoded columns for the top `n_common` most common diagnoses, admit diagnoses, and procedures. We"ll rename everything else to "Other"
"""

# Converting all these columns to strings to preserve categorical nature
df_train[cols_code + cols_physician] = df_train[cols_code + cols_physician].astype(str)

# Parameter for how many most common codes/physicians to turn into features
# Ideally should be different per type but ignoring that for now
n_common = 10

all_admit_diagnosis = [code for code in list(df_train[col_admit_diagnosis].values) if code != "nan"]
unique_admit_diagnosis = set(all_admit_diagnosis)
counter_admit_diagnosis = collections.Counter(all_admit_diagnosis)

print(f"Number of unique admit diagnosis codes: {len(unique_admit_diagnosis)}")
print(f"{n_common} most common unique admit diagnosis codes: {counter_admit_diagnosis.most_common(n_common)}")

all_diagnosis = [code for code in list(df_train[cols_diagnosis].values.ravel("K")) if code != "nan"]
unique_diagnosis = set(all_diagnosis)
counter_diagnosis = collections.Counter(all_diagnosis)

print(f"Number of unique diagnosis codes: {len(unique_diagnosis)}")
print(f"{n_common} most common unique diagnosis codes: {counter_diagnosis.most_common(n_common)}")

all_procedure = [code for code in list(df_train[cols_procedure].values.ravel("K")) if code != "nan"]
unique_procedure = set(all_procedure)
counter_procedure = collections.Counter(all_procedure)

print(f"Number of unique procedure codes: {len(unique_procedure)}")
print(f"{n_common} most common unique procedure codes: {counter_procedure.most_common(n_common)}")

all_physician = [code for code in list(df_train[cols_physician].values.ravel("K")) if code != "nan"]
unique_physician = set(all_physician)
counter_physician = collections.Counter(all_physician)

print(f"Number of unique physician codes: {len(unique_physician)}")
print(f"{n_common} most common unique physician codes: {counter_physician.most_common(n_common)}")

"""Let's also find out the most common for each of the above where "PotentialFraud" is "Yes"."""

df_fraud_yes = df_train.loc[df_train["PotentialFraud"] == "Yes"]

all_fraud_admit_diagnosis = [code for code in list(df_fraud_yes[col_admit_diagnosis].values) if code != "nan"]
counter_fraud_admit_diagnosis = collections.Counter(all_fraud_admit_diagnosis)

print(f"{n_common} most common admit diagnosis codes where potential fraud was detected: {counter_fraud_admit_diagnosis.most_common(n_common)}")

all_fraud_diagnosis = [code for code in list(df_fraud_yes[cols_diagnosis].values.ravel("K")) if code != "nan"]
counter_fraud_diagnosis = collections.Counter(all_fraud_diagnosis)

print(f"{n_common} most common diagnosis codes where potential fraud was detected: {counter_fraud_diagnosis.most_common(n_common)}")

all_fraud_procedure = [code for code in list(df_fraud_yes[cols_procedure].values.ravel("K")) if code != "nan"]
counter_fraud_procedure = collections.Counter(all_fraud_procedure)

print(f"{n_common} most common procedure codes where potential fraud was detected: {counter_fraud_procedure.most_common(n_common)}")

all_fraud_physician = [code for code in list(df_fraud_yes[cols_physician].values.ravel("K")) if code != "nan"]
counter_fraud_physician = collections.Counter(all_fraud_physician)

print(f"{n_common} most common physician codes where potential fraud was detected: {counter_fraud_physician.most_common(n_common)}")

# Make column names for one-hot columns (most common + "Other")
one_hot_admit_diagnosis = list(set(["AdmitDiagnosis_" + pair[0] for pair in counter_admit_diagnosis.most_common(n_common)] + ["AdmitDiagnosis_" + pair[0] for pair in counter_fraud_admit_diagnosis.most_common(n_common)] + ["AdmitDiagnosis_Other"]))
one_hot_admit_diagnosis.sort()
one_hot_diagnosis = list(set(["Diagnosis_" + pair[0] for pair in counter_diagnosis.most_common(n_common)] + ["Diagnosis_" + pair[0] for pair in counter_fraud_diagnosis.most_common(n_common)] + ["Diagnosis_Other"]))
one_hot_diagnosis.sort()
one_hot_procedure = list(set(["Procedure_" + pair[0] for pair in counter_procedure.most_common(n_common)] + ["Procedure_" + pair[0] for pair in counter_fraud_procedure.most_common(n_common)] + ["Procedure_Other"]))
one_hot_procedure.sort()
one_hot_physician = list(set(["Physician_" + pair[0] for pair in counter_physician.most_common(n_common)] + ["Physician_" + pair[0] for pair in counter_fraud_physician.most_common(n_common)] + ["Physician_Other"]))
one_hot_physician.sort()

# # Non-one hot columns are to be classified as "Other"
# other_admit_diagnosis = [pair[0] for pair in counter_admit_diagnosis.most_common()[n_common:]] + ["nan"]
# other_diagnosis = [pair[0] for pair in counter_diagnosis.most_common()[n_common:]] + ["nan"]
# other_procedure = [pair[0] for pair in counter_procedure.most_common()[n_common:]] + ["nan"]
# other_physician = [pair[0] for pair in counter_physician.most_common()[n_common:]] + ["nan"]

# Non-one hot columns are to be classified as "Other"
other_admit_diagnosis = [pair[0] for pair in counter_admit_diagnosis.most_common() if pair[0] not in [x.split("_")[1] for x in one_hot_admit_diagnosis]] + ["nan"]
other_diagnosis = [pair[0] for pair in counter_diagnosis.most_common()[n_common:] if pair[0] not in [x.split("_")[1] for x in one_hot_diagnosis]] + ["nan"]
other_procedure = [pair[0] for pair in counter_procedure.most_common()[n_common:] if pair[0] not in [x.split("_")[1] for x in one_hot_procedure]] + ["nan"]
other_physician = [pair[0] for pair in counter_physician.most_common()[n_common:] if pair[0] not in [x.split("_")[1] for x in one_hot_physician]] + ["nan"]

# Set values not in n_common most common values to "Other"
df_train.loc[df_train[col_admit_diagnosis].isin(other_admit_diagnosis), col_admit_diagnosis] = "Other"
for col in cols_diagnosis:
    df_train.loc[df_train[col].isin(other_diagnosis), col] = "Other"
for col in cols_procedure:
    df_train.loc[df_train[col].isin(other_procedure), col] = "Other"
for col in cols_physician:
    df_train.loc[df_train[col].isin(other_physician), col] = "Other"

# One-hot encoding
# For all columns of diagnosis, procedure, and physician:
# You stack them and get dummies, then aggregate to get max() of groups (max is True if any True)
df_train[one_hot_admit_diagnosis] = pd.get_dummies(df_train[col_admit_diagnosis])
df_train[one_hot_diagnosis] = pd.get_dummies(df_train[cols_diagnosis].stack()).groupby(level=0).max()
df_train[one_hot_procedure] = pd.get_dummies(df_train[cols_procedure].stack()).groupby(level=0).max()
df_train[one_hot_physician] = pd.get_dummies(df_train[cols_physician].stack()).groupby(level=0).max()

"""Let"s view samples of our new columns"""

df_train[[col_admit_diagnosis] + one_hot_admit_diagnosis][df_train["AdmitDiagnosis_78605"]].head()

df_train[cols_diagnosis + one_hot_diagnosis][df_train["Diagnosis_2724"]].head()

df_train[cols_procedure + one_hot_procedure][df_train["Procedure_66.0"]].head()

df_train[cols_physician + one_hot_physician][df_train["Physician_PHY337425"]].head()

"""Let"s remove all the original columns"""

df_train = df_train.drop(cols_code + cols_physician, axis=1)

df_train.info()

"""#### Date Columns

We want:

*   Age at time claim starts
*   Length of claim
*   Living or dead

Then we can drop these columns




"""

# Converting dates to datetime
df_train["DOB"] = pd.to_datetime(df_train["DOB"])
df_train["ClaimStartDt"] = pd.to_datetime(df_train["ClaimStartDt"])
df_train["ClaimEndDt"] = pd.to_datetime(df_train["ClaimEndDt"])

df_train["Living"] = df_train["DOD"].isna()
df_train["Age"] = (df_train["ClaimStartDt"] - df_train["DOB"]).dt.days
df_train["ClaimLength"] = (df_train["ClaimEndDt"] - df_train["ClaimStartDt"]).dt.days

df_train = df_train.drop(["DOB", "DOD", "ClaimStartDt", "ClaimEndDt"], axis=1)
df_train.info()

"""#### Other Columns

"DeductibleAmtPaid" is the only column left with null values. These are very few, and we care about the payment done. So let"s drop them.
"""

df_train = df_train[~df_train["DeductibleAmtPaid"].isna()]

"""Let"s drop the benefactor ID and provider ID as there are too many unique values."""

df_train = df_train.drop(["BeneID", "Provider"], axis=1)

"""Let"s have a look at the uniques present in the benefactor type, gender, state, county, and race columns, as we can expect them to have more than two categories."""

df_train[["BeneType", "Gender", "Race", "State", "County"]].nunique()

"""We can still use the benefactor type and race categories. There are too many categories in the state and county ones so let"s drop them for now. We convert the others to string to preserve categorical nature."""

df_train = df_train.drop(["County"], axis=1)
df_train[["BeneType", "Race", "Gender", "State"]] = df_train[["BeneType", "Race", "Gender", "State"]].astype(str)

"""Let"s find the number of uniques in each non-numeric column"""

df_non_numeric_cols = df_train.select_dtypes(exclude=["number"])
df_unique_values = pd.DataFrame({"Uniques": df_non_numeric_cols.nunique()}, index=df_non_numeric_cols.columns)
df_unique_values

"""The diagnosis and procedure columns have only one unique so we can drop them. Let"s one-hot encode the race and state, and drop them too."""

import warnings
warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)

df_train[[f"Race_{i}" for i in range(df_unique_values.loc["Race"].values[0])]] = pd.get_dummies(df_train["Race"])

one_hot_state = [f"State_{i}" for i in range(df_unique_values.loc["State"].values[0])]

df_train[one_hot_state] = pd.get_dummies(df_train["State"])
df_train = df_train.drop(["Diagnosis_Other", "Procedure_Other", "Race"], axis=1)

df_train.head()

"""#### Feature Scaling

Let"s standardise our numeric columns. We will preserve this scaler for use in the test set.
"""

scaler = StandardScaler()

df_numeric_columns = df_train.select_dtypes(include=["number"])
df_train[df_numeric_columns.columns] = scaler.fit_transform(df_train[df_numeric_columns.columns])

df_train.head()

"""#### Test Set

Our training set is ready. Ideally we would want to functionalise all this in development, but for now let"s just adapt the changes to the test data. We will use the training dataset for things like most frequent codes because we don"t have access to the test data at the start.
"""

dfs[6]["BeneType"] = "Inpatient"
dfs[7]["BeneType"] = "Outpatient"

df_test = pd.concat([dfs[6].drop(columns=unique_cols_inpatient), dfs[7]], axis=0).merge(dfs[5], how="left", on="BeneID").merge(dfs[4], how="left", on="Provider").reset_index(drop=True)
df_test.set_index("ClaimID", inplace=True)

df_test["NumDiagnosis"] = df_test[cols_diagnosis].count(axis=1)
df_test["NumProcedures"] = df_test[cols_procedure].count(axis=1)
df_test["NumPhysicians"] = df_test[cols_physician].count(axis=1)
df_test["DiagnosisMatch"] = df_test[cols_diagnosis].isin(df_test[col_admit_diagnosis]).any(axis=1)
df_test["PhysicianMatch"] = df_test[cols_physician].nunique(axis=1) < 3

df_test[cols_code + cols_physician] = df_test[cols_code + cols_physician].astype(str)

# Note the difference here because we want to see what"s not in the one-hot list, as there may be yet unseen codes in the test set
df_test.loc[~df_test[col_admit_diagnosis].isin([x.split("_")[1] for x in one_hot_admit_diagnosis]), col_admit_diagnosis] = "Other"
for col in cols_diagnosis:
    df_test.loc[~df_test[col].isin([x.split("_")[1] for x in one_hot_diagnosis]), col] = "Other"
for col in cols_procedure:
    df_test.loc[~df_test[col].isin([x.split("_")[1] for x in one_hot_procedure]), col] = "Other"
for col in cols_physician:
    df_test.loc[~df_test[col].isin([x.split("_")[1] for x in one_hot_physician]), col] = "Other"

df_test_admit_diagnosis = pd.get_dummies(df_test[col_admit_diagnosis])
df_test_diagnosis = pd.get_dummies(df_test[cols_diagnosis].stack()).groupby(level=0).max()
df_test_procedure = pd.get_dummies(df_test[cols_procedure].stack()).groupby(level=0).max()
df_test_physician = pd.get_dummies(df_test[cols_physician].stack()).groupby(level=0).max()
df_test_state = pd.get_dummies(df_test["State"])

one_hot_admit_diagnosis_no_prefix = [x.split("_")[1] for x in one_hot_admit_diagnosis]
one_hot_diagnosis_no_prefix = [x.split("_")[1] for x in one_hot_diagnosis]
one_hot_procedure_no_prefix = [x.split("_")[1] for x in one_hot_procedure]
one_hot_physician_no_prefix = [x.split("_")[1] for x in one_hot_physician]
one_hot_state_no_prefix = [x.split("_")[1] for x in one_hot_state]

# Set missing values to False
for col in one_hot_admit_diagnosis_no_prefix:
    if col not in df_test_admit_diagnosis.columns:
        df_test_admit_diagnosis[col] = False

for col in one_hot_diagnosis_no_prefix:
    if col not in df_test_diagnosis.columns:
        df_test_diagnosis[col] = False

for col in one_hot_procedure_no_prefix:
    if col not in df_test_procedure.columns:
        df_test_procedure[col] = False

for col in one_hot_physician_no_prefix:
    if col not in df_test_physician.columns:
        df_test_physician[col] = False

for col in one_hot_state_no_prefix:
    if col not in df_test_state.columns:
        df_test_state[col] = False

# Reorder columns
df_test_admit_diagnosis = df_test_admit_diagnosis[one_hot_admit_diagnosis_no_prefix]
df_test_diagnosis = df_test_diagnosis[one_hot_diagnosis_no_prefix]
df_test_procedure = df_test_procedure[one_hot_procedure_no_prefix]
df_test_physician = df_test_physician[one_hot_physician_no_prefix]
df_test_state = df_test_state[one_hot_state_no_prefix]

df_test[one_hot_admit_diagnosis] = df_test_admit_diagnosis
df_test[one_hot_diagnosis] = df_test_diagnosis
df_test[one_hot_procedure] = df_test_procedure
df_test[one_hot_physician] = df_test_physician
df_test[one_hot_state] = df_test_state

df_test = df_test.drop(cols_code + cols_physician, axis=1)

df_test["DOB"] = pd.to_datetime(df_test["DOB"])
df_test["ClaimStartDt"] = pd.to_datetime(df_test["ClaimStartDt"])
df_test["ClaimEndDt"] = pd.to_datetime(df_test["ClaimEndDt"])

df_test["Living"] = df_test["DOD"].isna()
df_test["Age"] = (df_test["ClaimStartDt"] - df_test["DOB"]).dt.days
df_test["ClaimLength"] = (df_test["ClaimEndDt"] - df_test["ClaimStartDt"]).dt.days

df_test = df_test.drop(["DOB", "DOD", "ClaimStartDt", "ClaimEndDt", "BeneID", "Provider", "County", "Diagnosis_Other", "Procedure_Other"], axis=1)
df_test = df_test[~df_test["DeductibleAmtPaid"].isna()]

df_test[["BeneType", "Race", "Gender", "State"]] = df_test[["BeneType", "Race", "Gender", "State"]].astype(str)
df_test[[f"Race_{i}" for i in range(df_unique_values.loc["Race"].values[0])]] = pd.get_dummies(df_test["Race"])
df_test = df_test.drop(["Race"], axis=1)

"""Scaling data"""

df_test[df_numeric_columns.columns] = scaler.transform(df_test[df_numeric_columns.columns])

df_test.info()

"""Again, all null values are removed

## Model Training

We will consider "PotentialFraud" as the target variable and make a dataset for training and validation. We can get rid of the claim ID at this stage. Using PCA to lower dimensionality after facing memory issues. We will lose interpretability as a result.
"""

n_components = 10*n_common
pca = PCA(n_components=n_components)

type(df_train.loc[:, df_train.columns == "PotentialFraud"])

y_train_val_series = df_train.loc[:, df_train.columns == "PotentialFraud"]["PotentialFraud"].map({"No": 0, "Yes": 1})

y_train_val = y_train_val_series.values

y_train_val

"""Use first block for PCA:"""

# Using pd.get_dummies() again because it will automatically one hot encode all string columns and drop the originals
# Turning everything to np arrays
x_train_val = pca.fit_transform(pd.get_dummies(df_train.loc[:, df_train.columns != "PotentialFraud"].copy()))
x_test = pca.transform(pd.get_dummies(df_test[df_train.loc[:, df_train.columns != "PotentialFraud"].columns].copy()))

# x_train_val = pd.get_dummies(df_train.loc[:, df_train.columns != "PotentialFraud"].copy()).values
# x_test = pd.get_dummies(df_test[df_train.loc[:, df_train.columns != "PotentialFraud"].columns].copy())

"""Applying random oversampling to help balance out the dataset"""

# oversampling = RandomOverSampler(sampling_strategy="minority")

# x_train_val.shape

# y_train_val.shape

# x_train_val , y_train_val = oversampling.fit_resample(x_train_val, y_train_val)

x_train_val.shape

y_train_val.shape

x_test.shape

"""#### Model Selection

Settings
"""

# Hyperparameters
params = {
    # "Logistic Regression": {
    #     "C": [0.1, 1, 10],
    #     "penalty": ["l2"],
    #     "solver": ["lbfgs"]
    # },
    "Logistic Regression": {
        "C": [0.1],
        "penalty": ["l2"],
        "solver": ["lbfgs"]
    },
    "Random Forest": {
        "n_estimators": [100, 200],
        "max_depth": [10, 20, None],
        "min_samples_split": [2, 5],
        "min_samples_leaf": [1, 2],
        "max_features": ["log", "sqrt"]
    },
    # "XGBoost": {
    #     "n_estimators": [100, 200],
    #     "max_depth": [3, 6, 10],
    #     "learning_rate": [0.01, 0.1],
    #     "subsample": [0.8, 1.0],
    #     "colsample_bytree": [0.8, 1.0]
    # },
    "XGBoost": {
        "n_estimators": [100],
        "max_depth": [6],
        "learning_rate": [0.01],
        "subsample": [0.8],
        "colsample_bytree": [0.8]
    },
    "LightGBM": {
        "n_estimators": [100, 200],
        "learning_rate": [0.01, 0.1],
        "num_leaves": [31, 64],
        "max_depth": [-1, 10, 20]
    },
    "CatBoost": {
        "iterations": [100, 200],
        "learning_rate": [0.01, 0.1],
        "depth": [6, 10],
        "l2_leaf_reg": [1, 3]
    },
    "SVM": {
        "C": [0.1, 1, 10],
        "kernel": ["linear", "rbf"],
        "gamma": ["scale", "auto"]
    },
    "Neural Network": {
        "hidden_layer_sizes": [(50,), (100,), (150,)],
        "activation": ["relu", "tanh"],
        "solver": ["adam"],
        "max_iter": [300]
    }
}

# Models
models = {
    "Logistic Regression": LogisticRegression(random_state=7, max_iter=1000),
    # "Random Forest": RandomForestClassifier(random_state=7),
    # "XGBoost": XGBClassifier(eval_metric="logloss", random_state=7),
    # "LightGBM": LGBMClassifier(random_state=7),
    # "CatBoost": CatBoostClassifier(verbose=0, random_state=7),
    # "SVM": SVC(random_state=7),
    # "Neural Network": MLPClassifier(max_iter=300, random_state=7)
}

# To store training results
model_results = {}

"""#### Model Evaluation

Set up k-fold cross-validation
"""

kfold_cv = KFold(n_splits=5, shuffle=True)

"""This function will evaluate each model and output a grid with metrics"""

def evaluate_model(model, model_name, params_dict):
    # Hyperparameter tuning
    grid_search = GridSearchCV(estimator=model, param_grid=params_dict, cv=kfold_cv, n_jobs=-1, scoring="accuracy", verbose=10)
    grid_search.fit(x_train_val, y_train_val)

    # Find best model
    best_model = grid_search.best_estimator_

    # Get metrics
    accuracies = cross_val_score(best_model, x_train_val, y_train_val, cv=kfold_cv, scoring="accuracy")
    precisions = cross_val_score(best_model, x_train_val, y_train_val, cv=kfold_cv, scoring="precision")
    recalls = cross_val_score(best_model, x_train_val, y_train_val, cv=kfold_cv, scoring="recall")
    f1_scores = cross_val_score(best_model, x_train_val, y_train_val, cv=kfold_cv, scoring="f1")
    roc_auc_scores = cross_val_score(best_model, x_train_val, y_train_val, cv=kfold_cv, scoring="roc_auc")

    model_results[model_name] = {
        "Best Hyperparameters": grid_search.best_params_,
        "Mean Accuracy": np.mean(accuracies),
        "Mean Precision": np.mean(precisions),
        "Mean Recall": np.mean(recalls),
        "Mean F1-Score": np.mean(f1_scores),
        "Mean ROC AUC": np.mean(roc_auc_scores),
        "Accuracy Scores": accuracies,
        "Precision Scores": precisions,
        "Recall Scores": recalls,
        "F1-Score": f1_scores,
        "ROC AUC Scores": roc_auc_scores
    }

    # Predict on the test set using the best model
    y_pred = best_model.predict(x_test)

    return y_pred

"""Evaluating model and displaying results"""

preds_dict = {} # stores output predictions for each model
for model_name, model in models.items():
    print(f"Evaluating {model_name}...")
    preds_dict[model_name] = evaluate_model(model, model_name, params[model_name])

df_results = pd.DataFrame(model_results).T
df_results = df_results.sort_values(by="Mean Accuracy", ascending=False)

print("\nModel Comparison Results (using Cross-Validation):")
print(df_results[["Best Hyperparameters", "Mean Accuracy", "Mean Precision", "Mean Recall", "Mean F1-Score", "Mean ROC AUC"]])

print("\nDetailed Per Fold Scores (Accuracy, Precision, Recall, F1-Score, ROC AUC):")
print(df_results[["Accuracy Scores", "Precision Scores", "Recall Scores", "F1-Score", "ROC AUC Scores"]])

df_results

preds_dict["Logistic Regression"]

len(preds_dict["Logistic Regression"])

df_test.index

results = pd.DataFrame({"PotentialFraud": preds_dict["Logistic Regression"]}, index=df_test.index)

results.head()

results.to_csv("results_on_unseen.csv")